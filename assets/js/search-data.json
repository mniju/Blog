{
  
    
        "post0": {
            "title": "An Introduction to PPO",
            "content": "An Introduction to PPO . Introduction: . PPO stands for Proximal Policy Optimization. Its a Policy gradient method for Reinforcement Learning(RL). It has much better performance than the TRPO (Trust Region Policy Optimization) but very simpler to Implement, more general and have better sample complexity. . Paper: https://arxiv.org/pdf/1707.06347.pdf . Here I cover the basic RL setup , then explain Policy Gradients and then using it to build the Intuition for Importance sampling and then the Policy Bounding. Basic math equations are covered as it cant be avoided altogether to understand Policy gradient and PPO . . RL Setup: . A standard RL setup consists of . Agent | Environment | . The Agent Interacts with the Environment by taking an action and collects the rewards and observes the next state of the environment. The environment is assumed to be fully observable so that we can formulate this as MDP. . . # This can be shown as , observation, reward,_ = env.step(action) . The Agent interacts with the environment in discrete timesteps ‘t’. For each time step, the agent receives an observation sts_tst​ , selects an action ata_tat​ ,following the policy π(at∣st) pi(a_t|s_t)π(at​∣st​) . The Agent receives a scalar reward rtr_trt​ and transitions to the next state st+1s_{t+1}st+1​ . Policy π piπ is the mapping of the probability of different actions for a state . The Returns from a state is defined as the sum of discounted future Rewards . Rt=∑i=tTγ(i−t)r(si,ai)R_t = sum ^T_ {i=t} gamma^{(i-t)}r(s_i,a_i)Rt​=i=t∑T​γ(i−t)r(si​,ai​) . The Objective of Reinforcement Learning is to Maximize Returns. That’s it ! . Policy Gradients: . The Policy Gradient Methods try to model a Policy that will maximize the expected Rewards. The Policy πθ(a∣s) pi_ theta(a|s)πθ​(a∣s) is usually learnt by a function approximator- where θ thetaθ is the parametrized network. . The Objective is : . maximizeθEπθ[∑t=0T−1γtrt]maximize_ theta mathbb E_{ pi theta} left[ sum_{t=0}^{T-1} gamma^tr_t right]maximizeθ​Eπθ​[t=0∑T−1​γtrt​] . Maximize the Expected rewards computed from a trajectory , generated by the policy πθ pi_ thetaπθ​. . To find the best θ thetaθ for any function f(x)f(x)f(x) we need to do stochastic gradient ascent on θ thetaθ . θ←θ+α▽f(x) theta leftarrow theta + alpha triangledown f(x)θ←θ+α▽f(x) . Here f(x)f(x)f(x) is our sum of rewards objective in our previous equation. so we need to find . ▽Eπθ[∑t=0T−1γtrt]⇢(1) triangledown mathbb E_{ pi theta} left[ sum_{t=0}^{T-1} gamma^tr_t right] dashrightarrow (1)▽Eπθ​[t=0∑T−1​γtrt​]⇢(1) . How to Calculate ▽θE[f(x)] triangledown_ theta mathbb E left[ f(x) right]▽θ​E[f(x)] . Using Log Derivative Trick . Mathematical expectation, also known as the expected value, is the summation or integration of a possible values from a random variable. It is also known as the product of the probability of an event occurring, and the value corresponding with the actual observed occurrence of the event . So the expected value is the sum of: [(each of the possible outcomes) × (the probability of the outcome occurring)]. . The Expectation of f(x)f(x)f(x) where x is a random variable ,under the distribution ppp . Ex∼p(x)[f(x)]=∫p(x)f(x)dx mathbb{E}_{x sim p(x)} left[ f(x) right] = int p(x)f(x)dxEx∼p(x)​[f(x)]=∫p(x)f(x)dx . Expanding for E[f(x)] mathbb E left[ f(x) right]E[f(x)] . ▽θE[f(x)]=▽θ∫pθ(x)f(x)dx bigtriangledown_ theta mathbb E left[ f(x) right] = bigtriangledown_ theta int p_ theta(x)f(x) dx▽θ​E[f(x)]=▽θ​∫pθ​(x)f(x)dx . Multiply and divide by p(x) . ∫pθ(x)▽pθ(x)pθ(x)f(x)dx int p_ theta(x) frac { bigtriangledown p_ theta(x)} {p_ theta(x)} f(x)dx∫pθ​(x)pθ​(x)▽pθ​(x)​f(x)dx . Using the log formulae ▽θlog(z)=1z▽θz triangledown_ theta log(z) = frac 1 z triangledown_ theta z▽θ​log(z)=z1​▽θ​z . ∫pθ(x)▽θlogpθ(x)f(x)dx int p_ theta(x) bigtriangledown_ theta log p_ theta(x)f(x)dx∫pθ​(x)▽θ​logpθ​(x)f(x)dx . Again rewriting using Expectation: . ▽θE[f(x)]=E[f(x)▽θlogpθ(x)]⇢(2) triangledown_ theta mathbb E left[ f(x) right]= mathbb E left[ f(x) bigtriangledown_ theta logp_ theta(x) right] dashrightarrow(2)▽θ​E[f(x)]=E[f(x)▽θ​logpθ​(x)]⇢(2) . We will replace the x with the trajectory τ tauτ .Next step is to find the log probability of the trajectory τ tauτ. . Computation of logpθ(τ)log p_ theta( tau)logpθ​(τ) . Let, . μ muμ - starting state distribution . πθ pi_ thetaπθ​ - Policy - probability of taking an action given a state . P - Dynamics of Environment . Trajectory = Initial State + Further Transitions from the initial state based on actions taken following policy π piπ. . We can notice that when taking the gradients , the dynamics disappear and thus , Policy gradients doesn’t need to know the environment Dynamics. . ▽θlog⁡pθ(τ)=▽log⁡(μ(s0)∏t=0T−1πθ(at∣st)P(st+1∣st,at))=▽θ[log⁡μ(s0)+∑t=0T−1(log⁡πθ(at∣st)+log⁡P(st+1∣st,at))]=▽θ∑t=0T−1log⁡πθ(at∣st)⇢(3) triangledown_ theta log p_ theta( tau) = triangledown log left( mu(s_0) prod_{t=0}^{T-1} pi_ theta(a_t|s_t)P(s_{t+1}|s_t,a_t) right) = triangledown_ theta left[ log mu(s_0)+ sum_{t=0}^{T-1} ( log pi_ theta(a_t|s_t) + log P(s_{t+1}|s_t,a_t)) right] = triangledown_ theta sum_{t=0}^{T-1} log pi_ theta(a_t|s_t) dashrightarrow(3)▽θ​logpθ​(τ)=▽log(μ(s0​)t=0∏T−1​πθ​(at​∣st​)P(st+1​∣st​,at​))=▽θ​[logμ(s0​)+t=0∑T−1​(logπθ​(at​∣st​)+logP(st+1​∣st​,at​))]=▽θ​t=0∑T−1​logπθ​(at​∣st​)⇢(3) . Objective: ▽Eπθ[∑t=0T−1γtrt] triangledown mathbb E_{ pi theta} left[ sum_{t=0}^{T-1} gamma^tr_t right]▽Eπθ​[∑t=0T−1​γtrt​] . Substituting (3) in (2) and then (2) in (1) . ∇θEτ∼πθ[R(τ)]=Eτ∼πθ[R(τ)⋅∇θ(∑t=0T−1log⁡πθ(at∣st))]⇢(4) nabla_ theta mathbb{E}_{ tau sim pi_ theta}[R( tau)] = mathbb{E}_{ tau sim pi_ theta} left[R( tau) cdot nabla_ theta left( sum_{t=0}^{T-1} log pi_ theta(a_t|s_t) right) right] dashrightarrow(4)∇θ​Eτ∼πθ​​[R(τ)]=Eτ∼πθ​​[R(τ)⋅∇θ​(t=0∑T−1​logπθ​(at​∣st​))]⇢(4) . R(τ)R( tau)R(τ) - The reward function that we want to maximize. . The Expectation of the rewards over the trajectory following the policy π piπ . This is the Objective in Policy Gradient problem . As expained in Pong From Pixels . This equation is telling us how we should shift the distribution (through its parameters θ thetaθ) if we wanted its samples to achieve higher scores, as judged by the Reward Function. It’s telling us that we should take this direction given by gradient of log⁡πθ(at∣st) log pi_ theta(a_t|s_t)logπθ​(at​∣st​) (which is a vector that gives the direction in the parameter space θ thetaθ) and multiply onto it the scalar-valued score Rewards. This will make it so that samples that have a higher score will “tug” on the probability density stronger than the samples that have lower score, so if we were to do an update based on several samples from p the probability density would shift around in the direction of higher scores, making highly-scoring samples more likely. . We see that the Rewards (scalar values) is multiplied with the Gradient of the Log probability of the action , given state. The Gradient Vector point the direction we should move to optimize the objective. The Gradient is factored by the Rewards. . This enables Probability density function moves towards the action probabilities that creates high score. . Good Stuff is made More Likely. . Bad stuff is made less likely . . . CS285: Trajectories with Good and Bad Rewards . . As explained in GAE paper, these are some of the variations of this Policy Gradient where the Rewards function is replaced with other expressions for bias Variance Trade-off . . GAE Paper : https://arxiv.org/pdf/1506.02438.pdf . Importance Sampling in Policy Gradients . Policy Gradient is On-Policy - Every time we generate a policy we need to generate own samples . So the steps are . Create Sample with the current Policy. | Find the Gradient of the objective. | Take a gradient step for Optimization. | This is because, The objective is the Expectation of the grad log over the current Trajectory generated by the current Policy . . . CS285:https://youtu.be/Ds1trXd6pos?t=3415 . Once we take the gradient over the policy , the policy is changed and we cannot use the Trajectory generated previously. We need to new samples again with the current policy. . This is shown below . CS285:https://youtu.be/Ds1trXd6pos?t=3415 . What if we don’t have the samples from the policy πθ(τ) pi_ theta( tau)πθ​(τ) instead we have π‾(τ) overline pi( tau)π(τ) . Importance Sampling comes into play here! . Importance Sampling: . From Wikepdia, . In statistics, importance sampling is a general technique for estimating properties of a particular distribution, while only having samples generated from a different distribution than the distribution of interest. . Expectation of random variable f(x)f(x)f(x) under distribution ppp in terms of distribution under qqq . The Expectation of a random variable f(x)f(x)f(x) under distribution ppp . Ex∼p(x)[f(x)]=∫p(x)f(x)dx mathbb{E}_{x sim p(x)} left[ f(x) right] = int p(x)f(x)dxEx∼p(x)​[f(x)]=∫p(x)f(x)dx . Multiplying and dividing by q(x)q(x)q(x) . =∫q(x)q(x)p(x)f(x)dx= int frac {q(x)} {q(x)} p(x)f(x)dx=∫q(x)q(x)​p(x)f(x)dx . Rearranging q(x)q(x)q(x) . =∫q(x)p(x)q(x)f(x)dx= int {q(x)} frac {p(x)} {q(x)} f(x)dx=∫q(x)q(x)p(x)​f(x)dx . This is equal to the Expectation under distribution qqq given by . Ex∼p(x)[f(x)]=Ex∼q(x)[p(x)q(x)f(x)] mathbb{E}_{x sim p(x)} left[ f(x) right]= mathbb{E}_{x sim q(x)} left[ frac {p(x)}{q(x)} f(x) right]Ex∼p(x)​[f(x)]=Ex∼q(x)​[q(x)p(x)​f(x)] . So Expectation under ppp for f(x)f(x)f(x) is equal to the Expectation under qqq with the ratio p(x)q(x) frac {p(x)}{q(x)}q(x)p(x)​ times f(x)f(x)f(x) . Plugging this for the old policy distribution π‾(τ) overline pi( tau)π(τ), The objective becomes . J(θ)=Eτ∼π‾(τ)[πθ(τ)π‾(τ)r(τ)]J( theta) = mathbb{E}_{ tau sim overline pi( tau)} left[ frac { pi_ theta( tau)}{ overline pi( tau)} r( tau) right]J(θ)=Eτ∼π(τ)​[π(τ)πθ​(τ)​r(τ)] . . Short Recap: . The original Objective . J(θ)=Eτ∼πθ(τ)[r(τ)]J( theta) = mathbb{E}_{ tau sim pi_ theta( tau)} left[ r( tau) right]J(θ)=Eτ∼πθ​(τ)​[r(τ)] . Estimating for the new parameters $ theta$’ with Importance Sampling . J(θ)′=Eτ∼πθ(τ)[πθ′(τ)πθ(τ)r(τ)]J( theta)&amp;#x27; = mathbb{E}_{ tau sim pi_ theta( tau)} left[ frac { pi_{ theta&amp;#x27;}( tau)}{ pi_ theta( tau)} r( tau) right]J(θ)′=Eτ∼πθ​(τ)​[πθ​(τ)πθ′​(τ)​r(τ)] . ∇θ′J(θ)′=Eτ∼πθ(τ)[∇θ′πθ′(τ)πθ(τ)r(τ)] nabla_{ theta&amp;#x27;}J( theta)&amp;#x27;= mathbb{E}_{ tau sim pi_ theta( tau)} left[ nabla_{ theta&amp;#x27;} frac { pi_{ theta&amp;#x27;}( tau)}{ pi_ theta( tau)} r( tau) right]∇θ′​J(θ)′=Eτ∼πθ​(τ)​[∇θ′​πθ​(τ)πθ′​(τ)​r(τ)] . Using identity ,πθ(τ)∇θlogπθ(τ)=∇θπθ(τ) pi_ theta( tau) nabla_ theta log pi_ theta( tau) = nabla_ theta pi _ theta( tau)πθ​(τ)∇θ​logπθ​(τ)=∇θ​πθ​(τ) . =Eτ∼πθ(τ)[πθ′(τ)πθ(τ)∇θ′logπθ′(τ)r(τ)]= mathbb{E}_{ tau sim pi_ theta( tau)} left[ frac { pi_{ theta&amp;#x27;}( tau)}{ pi_ theta( tau)} nabla_{ theta&amp;#x27;}log pi_{ theta&amp;#x27;}( tau)r( tau) right]=Eτ∼πθ​(τ)​[πθ​(τ)πθ′​(τ)​∇θ′​logπθ′​(τ)r(τ)] . ∇J(θ)=Eτ∼πθold(τ)[πθ(τ)πθold(τ)∇θlogπθ(τ)r(τ)] nabla J( theta) = mathbb{E}_{ tau sim pi_{ theta_{old}}( tau)} left[ frac { pi_{ theta}( tau)}{ pi_{ theta_{old}}( tau)} nabla_{ theta}log pi_{ theta}( tau)r( tau) right]∇J(θ)=Eτ∼πθold​​(τ)​[πθold​​(τ)πθ​(τ)​∇θ​logπθ​(τ)r(τ)] . Importance sampling enables us to use the samples from the old policy to calculate the Policy Gradient . Problems with Importance Sampling: . ∇J(θ)=Eτ∼πθold(τ)[πθ(τ)πθold(τ)∇θlogπθ(τ)r(τ)] nabla J( theta) = mathbb{E}_{ tau sim pi_{ theta_{old}}( tau)} left[ frac { pi_{ theta}( tau)}{ pi_{ theta_{old}}( tau)} nabla_{ theta}log pi_{ theta}( tau)r( tau) right]∇J(θ)=Eτ∼πθold​​(τ)​[πθold​​(τ)πθ​(τ)​∇θ​logπθ​(τ)r(τ)] . Expanding πθ(τ)πθold(τ) frac { pi_{ theta}( tau)}{ pi_{ theta_{old}}( tau)}πθold​​(τ)πθ​(τ)​ . . http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_13_advanced_pg.pdf . The Policy Gradient Objective with IS and Advantage Function is . ∇J(θ)=Eτ∼πθold(τ)[πθ(st,at)πθold(st,at)∇θlogπθ(st,at)A(st,at)] nabla J( theta) = mathbb{E}_{ tau sim pi_{ theta_{old}}( tau)} left[ frac { pi_{ theta}(s_t,a_t)}{ pi_{ theta_{old}}(s_t,a_t)} nabla_{ theta}log pi_{ theta}(s_t,a_t)A(s_t,a_t) right]∇J(θ)=Eτ∼πθold​​(τ)​[πθold​​(st​,at​)πθ​(st​,at​)​∇θ​logπθ​(st​,at​)A(st​,at​)] . High Variance in Importance Sampling : . VAR[X]=E[X2]−(E[X])2VAR[X]=E[X^2]-(E[X])^2VAR[X]=E[X2]−(E[X])2 . . The variance of the Importance Sampling Estimator depends on the ratio p(x)q(x) frac {p(x)}{q(x)}q(x)p(x)​. . As see in above equation for the ratio πθ(τ)πθold(τ) frac { pi_{ theta}( tau)}{ pi_{ theta_{old}}( tau)}πθold​​(τ)πθ​(τ)​, the probabilities are all multiplied and many small differences multiply to become a larger. . This ratio if its large ,may cause the gradients to explode . . This also means , we may need more sample data if the ratio is far from 1. . Unstable Step Updates: . The trajectories generated with the old policy , they may be having the states, that are not that interesting. May be they all have lesser rewards then that of the current Policy. . The new policy is dependent on the old policy . We need to use the old policy and make confident updates when we take a gradient step . Make the step small updates . Too large step means, performance Collapse | Too small ,progress very slow. | The right step changes depends where we are in the policy space | Adaptive learning rate - like Adam - doesn’t work well . So the interesting thing is here that the policies nearer in parameter space differs so much in the policy space. . This is because distance in Policy space and Parameter space are different. . . http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_13_advanced_pg.pdf . We need to find a policy update, that reflects the underlying structure of the policy space as a function of the parameter space. . Policy Bounds . Somehow we should bound this difference between these distributions (ie) the Old policy distribution $ pi_ theta$ and new policy πθ′ pi_{ theta&amp;#x27;}πθ′​ distribution . . We want an update step that is: . uses rollouts collected from the most recent policy as efficiently as possible | and takes steps that respect distance in policy space as opposed to distance in parameter space | . Relative Policy Performance Bounds: . We need to check the performance of one policy to the performance of another policy. . As explained in this Lecture . We want to produce a new policy update method, which is going to take the form of some kind of optimization problem as in local policy search which is a super class of policy gradient algorithm. We want to maximize some objective, subject to the constraint .. that we are going to say close to the previous policy. So we are going to figure out what that objective is going to be. . . Lπ(π′)L_ pi( pi&amp;#x27;)Lπ​(π′) is our new objective. We call this a surrogate objective. . Now,We can use trajectories sampled from the old policy along with the Advantage calculated from the old policy Trajectory. Still we need the new policy action probability , however we don’t want to rollout for the new policy to collect the rewards. . So what about the constraint/Bounds? . As seen from the above equation, Lπ(π′)L_ pi( pi&amp;#x27;)Lπ​(π′) is the Surrogate Objective. We maximize that surrogate objective , so as to reduce absolute value in the Left hand of the below equation. We do it such a way to keep the KL divergence in some limits. . . The policies should be bound by KL divergence. If policies are close in KL-divergence - the approximation is good ! . Kullback-Leibler Divergence: . Its a measure of difference between two distributions. The distance between two distributions P(x) and Q(x) given as . D​KL​​(p∣∣q)=∑i=1Np(xi​​)⋅(logp(x​i​​)−logq(x​i​​))D​KL​​(p∣∣q)=E[logp(x)−logq(x)]DKL(P∣∣Q)=∑xP(x)logP(x)Q(x)D​_{KL​​}(p∣∣q)= sum _{i=1}^N p(x_i​​)⋅(log p(x​_i​​)−log q(x​_i​​)) D​_{KL​​}(p∣∣q)=E[log p(x)−log q(x)] D_{KL}(P||Q) = sum_xP(x)log frac {P(x)}{Q(x)}D​KL​​​(p∣∣q)=i=1∑N​p(xi​​​)⋅(logp(x​i​​​)−logq(x​i​​​))D​KL​​​(p∣∣q)=E[logp(x)−logq(x)]DKL​(P∣∣Q)=x∑​P(x)logQ(x)P(x)​ . Its the expectation of the Logarithmic difference between the two probabilities P and Q. . . KL Divergence of Two policies π1 and π2  pi_1 space and space pi_2 spaceπ1​ and π2​  can be written as . DKL(π1∣∣π2)[s]=∑aϵAπ1(a∣s)logπ1(a∣s)π2(a∣s)D_{KL}( pi_1|| pi_2)[s] = sum_{a epsilon A} pi_1(a|s)log frac { pi_1(a|s)}{ pi_2(a|s)}DKL​(π1​∣∣π2​)[s]=aϵA∑​π1​(a∣s)logπ2​(a∣s)π1​(a∣s)​ . TRPO sneak peak: . The objective is . mθaximize E^t[πθ(st,at)πθold(st,at)A^t] underset{ theta}maximize mathbb{ hat E}_t left[ frac { pi_{ theta}(s_t,a_t)}{ pi_{ theta_{old}}(s_t,a_t)} hat A_t right]θm​aximize E^t​[πθold​​(st​,at​)πθ​(st​,at​)​A^t​] . So,we Maximize the objective , subjecting to condition, the KL Divergence between two policies are less than a value $ delta$. This can be written as . E^t[KL[πθold(⋅∣st),πθ(⋅∣st)]]≤δ mathbb{ hat E}_t left[ KL left[ { pi_{ theta_{old}}( cdot|s_t)},{ pi_{ theta}( cdot|s_t)} right] right] le deltaE^t​[KL[πθold​​(⋅∣st​),πθ​(⋅∣st​)]]≤δ . With Lagrangian Dual Trick, we write as unconstrained optimization problem . mθaximize E^t[πθ(st,at)πθold(st,at)A^t−βKL[πθold(⋅∣st),πθ(⋅∣st)]] underset{ theta}maximize mathbb{ hat E}_t left[ frac { pi_{ theta}(s_t,a_t)}{ pi_{ theta_{old}}(s_t,a_t)} hat A_t - beta KL left[ { pi_{ theta_{old}}( cdot|s_t)},{ pi_{ theta}( cdot|s_t)} right] right]θm​aximize E^t​[πθold​​(st​,at​)πθ​(st​,at​)​A^t​−βKL[πθold​​(⋅∣st​),πθ​(⋅∣st​)]] . Here penalty coefficient $ beta$ is constant value,Natural Policy Gradient is used , and additionally the computational Intensive Hessian Matrix. . . http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_13_advanced_pg.pdf . TRPO is explained in may places . This blog by Jonathan Hui explains more clearly. . Proximal Policy Optimization: . As in TRPO we try to constraint the new policy near to the old policy but ,without computing Natural Gradient. There are two variants. . Adaptive KL Penalty | . This solves a constraint problem similar to TRPO. PPO uses a soft penalty coefficient to penalize the KL divergence and its adjusted appropriately over the course of the training. This is a first order method . The Objective is : . LKLPEN(θ)=E^t[πθ(st,at)πθold(st,at)A^−βKL[πθold(⋅∣st),πθ(⋅∣st)]]L^{KLPEN}( theta)= mathbb{ hat E}_t left[ frac { pi_{ theta}(s_t,a_t)}{ pi_{ theta_{old}}(s_t,a_t)} hat A - beta KL left[ { pi_{ theta_{old}}( cdot|s_t)},{ pi_{ theta}( cdot|s_t)} right] right]LKLPEN(θ)=E^t​[πθold​​(st​,at​)πθ​(st​,at​)​A^−βKL[πθold​​(⋅∣st​),πθ​(⋅∣st​)]] . TRPO(Primal Dual descence strategy) alternates between update the policy parameters and Lagrange multipliers in the same optimization update iteration .However In PPO we keep the penalty coefficient constant for the whole section of optimization and then afterwards modify it.Compute, . d=E^t[KL[πθold(⋅∣st),πθ(⋅∣st)]]d = mathbb{ hat E}_t left[ KL left[ { pi_{ theta_{old}}( cdot|s_t)},{ pi_{ theta}( cdot|s_t)} right] right]d=E^t​[KL[πθold​​(⋅∣st​),πθ​(⋅∣st​)]] . If d&gt;dtarg×1.5,β←β×2d&gt;d_{targ} times1.5, beta leftarrow beta times 2d&gt;dtarg​×1.5,β←β×2. The KL divergence is larger than the target value . Increase the Penalty Coefficient . If d&lt;dtarg/1.5,β←β/2d&lt;d_{targ}/1.5, beta leftarrow beta/2d&lt;dtarg​/1.5,β←β/2. The KL divergence is too small than the target, probably lower the penalty coefficient. . . CS294: http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_13_advanced_pg.pdf . Clipped Objective | . This is much simpler than PPO with KL Penalty. . As usual , the objective function is : . mθaximize E^t[πθ(st,at)πθold(st,at)A^t] underset{ theta}maximize mathbb{ hat E}_t left[ frac { pi_{ theta}(s_t,a_t)}{ pi_{ theta_{old}}(s_t,a_t)} hat A_t right]θm​aximize E^t​[πθold​​(st​,at​)πθ​(st​,at​)​A^t​] . We define rt(θ)r_t( theta)rt​(θ) as the likelihood ratio . rt(θ)=πθ(st,at)πθold(st,at)r_t( theta) = frac { pi_{ theta}(s_t,a_t)}{ pi_{ theta_{old}}(s_t,a_t)}rt​(θ)=πθold​​(st​,at​)πθ​(st​,at​)​ . We just want to clip this ratio. . . https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-spring18/slides/cs885-lecture15b.pdf . We can see that the ratio r is clipped between $1+ epsilon$ and $1- epsilon$ where $ epsilon$ is the clipping hyperparameter . The Clipped objective is : . LCLIP(θ)=E^t[min(rt(θ)A^t,clip[rt(θ),1−ϵ,1+ϵ]A^t)]L^{CLIP}( theta)= mathbb{ hat E}_t left[min (r_t( theta) hat A_t , clip left[r_t( theta),1- epsilon , 1+ epsilon right] hat A_t ) right]LCLIP(θ)=E^t​[min(rt​(θ)A^t​,clip[rt​(θ),1−ϵ,1+ϵ]A^t​)] . We take the minimum between the unclipped value rt(θ)A^tr_t( theta) hat A_trt​(θ)A^t​ and the clipped value (clip[rt(θ),1−ϵ,1+ϵ]A^t)(clip left[r_t( theta),1- epsilon , 1+ epsilon right] hat A_t)(clip[rt​(θ),1−ϵ,1+ϵ]A^t​). This make the policy update to be more pessimistic and discourages from make abrupt changes in policy updates based on bigger/smaller rewards. . We don’t have any constraints, no Penalties .There is no KL divergence here , its much simpler and the clipping is easier to implement. . . We should note the partial trajectories and the minibatches update for a batch. . PPO practice: . When we use PPO network in an architecture like Actor Critic , (where Policy is actor and Value is critic ) , we use the following in the objective . 1.Clipped rewards(Surrogate Function) . Squared Error Loss (Critic) . | Entropy (To encourage exploration) . | . https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-spring18/slides/cs885-lecture15b.pdf . PPO Implementation: . There are many github repositories that has PPO implementation. There is one from openai/baselines ,famous pytorch implementation by ikostrikov , QunitinFettes, CleanRL , higgsfield/RL-Adventure etc.. . There is a nice blog post on PPO Implementation details .Please check this list for your implementation details. . I have implemented a PPO Notebook for continuous environment with the boiler plate code provided by Yandex. My implementation may not be perfect. . I will just highlight few items here. . Network Architecture: . The networks should use Orthogonal initialisation with parameter $ sqrt2$ and biases zero. . def layer_init(layer, std=np.sqrt(2), bias_const=0.0): &#39;&#39;&#39; https://github.com/vwxyzjn/cleanrl/blob/ 418bfc01fe69712c5b617d49d810a1df7f4f0c14/cleanrl/ppo_continuous_action.py#L221 &#39;&#39;&#39; nn.init.orthogonal_(layer.weight, std) nn.init.constant_(layer.bias, bias_const) return layer . The network is kind of Actor critic,wherein we use one network for policy and another one for values. The activation function is tanh. . class Model(nn.Module): def __init__(self,num_inputs,num_outputs): super(Model,self).__init__() self.fc1 = layer_init(nn.Linear(input_shape,64)) self.fc2 = layer_init(nn.Linear(64,64)) self.fc_Policy = layer_init(nn.Linear(64,n_actions)) self.fc_Value = layer_init(nn.Linear(64,1)) self.covariance = nn.Parameter(torch.zeros(1,n_actions)) def Policy_network(self,x): &#39;&#39;&#39; The network predicts the mean and covariance(log of standard deviation) &#39;&#39;&#39; x = torch.tanh(self.fc1(x)) x = torch.tanh(self.fc2(x)) mean = self.fc_Policy(x) logstd = self.covariance.expand_as(mean) return mean,logstd def Value_network(self,x): &#39;&#39;&#39; The network predicts the Value Function &#39;&#39;&#39; x = torch.tanh(self.fc1(x)) x = torch.tanh(self.fc2(x)) x = self.fc_Value(x) return x . This Model is wrapped with another Class Policy to call in the Model in two different conditions . When in Collecting partial Trajectories . This is called partial trajectories as we wont collect the trajectory until the end. But we just collect a fixed set of tuples {actions, log_probabilities,values} for the policy . | if not training: &#39;&#39;&#39; training=False -- Value Input is Observation Sample action for a Trajectory return {&quot;actions:&quot;,&quot;log_probs&quot;,&quot;values&quot;} &#39;&#39;&#39; with torch.no_grad(): x = torch.Tensor(inputs).unsqueeze(0) mean,logstd = self.model.Policy_network(x) std = torch.exp(logstd) distrib = Normal(mean,std) action = distrib.sample()[0] log_prob = distrib.log_prob(action).sum(1).view(-1).cpu().detach().numpy() value = self.model.Value_network(x).view(-1).cpu().detach().numpy() return {&quot;actions&quot;:action.detach().numpy(),&quot;log_probs&quot;:log_prob,&quot;values&quot;:value} . When in training . Just return the action distribution along with the values. This will be called when making every step update. . | else: &#39;&#39;&#39; training=True - - Policy &amp; Value Input is Observations return {&quot;distribution&quot;,&quot;values&quot;} &#39;&#39;&#39; x = torch.Tensor(inputs) mean,logstd = self.model.Policy_network(x) std = torch.exp(logstd) distrib = Normal(mean,std) value = self.model.Value_network(x) return {&quot;distribution&quot;:distrib,&quot;values&quot;:value} . Generalized Advantage Estimate GAE: . We use an Advantage Estimator that has two parameters $ gamma space and space lambda$ for bias-variance trade off as explained in this famous paper . We initialize all the values first . . We have the last observed state in the Trajectory . To get the values for that last state, we call the Network model . advantages = [] returns =[] lastgae = 0 rewards = trajectory[&quot;rewards&quot;] values = trajectory[&quot;values&quot;] dones = 1- trajectory[&quot;resets&quot;] #Get the latest state last_state = trajectory[&quot;state&quot;][&quot;latest_observation&quot;] # Output of the network for the &#39;next_state&#39; input network_output =self.policy.act(last_state, training=False) last_value = network_output[&quot;values&quot;] values = np.append(values,[last_value])# Append the next value . Next , we loop through to calculate the Advantage. We calculate the returns as advantage+values . # https://github.com/colinskow/move37/ # blob/f57afca9d15ce0233b27b2b0d6508b99b46d4c7f/ppo/ppo_train.py#L69 for step in reversed(range(len(rewards))): td_delta = rewards[step] + self.gamma * values[step+1] * dones[step] - values[step] advantage =lastgae= td_delta + self.gamma*self.lambda_*dones[step]*lastgae advantages.insert(0,advantage) returns.insert(0,advantage+values[step]) . Losses . Policy Loss . The clipped objective is . LCLIP(θ)=E^t[min(rt(θ)A^t,clip[rt(θ),1−ϵ,1+ϵ]A^t)]L^{CLIP}( theta)= mathbb{ hat E}_t left[min (r_t( theta) hat A_t , clip left[r_t( theta),1- epsilon , 1+ epsilon right] hat A_t ) right]LCLIP(θ)=E^t​[min(rt​(θ)A^t​,clip[rt​(θ),1−ϵ,1+ϵ]A^t​)] . Advantage is calculated from old policy. Ratio is calculated between new policy and old policy. . def policy_loss(self, trajectory, act): &quot;&quot;&quot; Computes and returns policy loss on a given trajectory. &quot;&quot;&quot; actions = torch.tensor(trajectory[&quot;actions&quot;]).to(device) old_log_probs = torch.tensor(trajectory[&quot;log_probs&quot;]).to(device).flatten() new_distrib = act[&quot;distribution&quot;] new_logprobs = new_distrib.log_prob(actions).sum(1) entropy = new_distrib.entropy().sum(1) self.entropy_loss = entropy.mean() ratio = torch.exp(new_logprobs - old_log_probs) surrogate1 = ratio * -torch.Tensor(trajectory[&quot;advantages&quot;]).to(device) surrogate2 = torch.clamp(ratio,1-self.cliprange,1+self.cliprange)*-torch.Tensor(trajectory[&quot;advantages&quot;]) policy_loss = torch.mean(torch.max(surrogate1,surrogate2)) return policy_loss . Value Loss: . We use clipped value loss function. . def value_loss(self, trajectory, act): &quot;&quot;&quot; Computes and returns value loss on a given trajectory. &quot;&quot;&quot; new_values = act[&quot;values&quot;].flatten() # returns(Target Values)= Advantage+value returns = torch.tensor(trajectory[&quot;value_targets&quot;]).to(device) #Advantage+value old_values = torch.tensor(trajectory[&quot;values&quot;]).to(device).flatten() # Old Values # Squared Error Loss v_loss1 =(returns - new_values ).pow(2) # Target_values - New_values clipped_values = old_values+ torch.clamp(new_values - old_values,-self.cliprange,self.cliprange) v_loss2 = (clipped_values - returns ).pow(2) # Target_values - clipped_values value_loss = 0.5*(torch.max(v_loss1,v_loss2)).mean() return value_loss . Total Loss: . total_loss = policy_loss + self.value_loss_coef * value_loss - self.entropy_coef*self.entropy_loss . Helpful Blogs: . https://spinningup.openai.com/en/latest/algorithms/ppo.html . | https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html . | https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/ . | https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-spring18/slides/cs885-lecture15b.pdf . | http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_13_advanced_pg.pdf . | https://www.stat.auckland.ac.nz/~fewster/325/notes/ch3.pdf | Pong from Pixels | Video Tutorials: . CS 285 - Berkley Deep RL Lectures especially this on TRPO and PPO | PPO from CS 885 Waterloo Deep RL course . | Coding Tutorial from schoolofai |",
            "url": "https://mniju.github.io/Blog/proximal%20policy%20optimization/2020/07/24/An-Introduction-to-PPO.html",
            "relUrl": "/proximal%20policy%20optimization/2020/07/24/An-Introduction-to-PPO.html",
            "date": " • Jul 24, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Notes on Q learning",
            "content": "Introduction . Q learning is one of the sucessful techniques in Reinforcement Learning .It was initially introduced in 1989 by Watkins. Q learning came to light when Deepmind used it to play Atari games with superhuman perfomance in 2015. . In General,Q learning is a model free learning .Its an off policy TD Control. . In this post , We will see a small RL Introduction , then into Qlearning and end with Qlearning &amp; Sarsa Comparision. . The jupyter notebook comparision of Q learning and SARSA for the cliff walker enviornment can be found here. . Reinforcement Learning Basic Setup . . Simply , In Reinforcement Learning, the Agent acts on the Enviornment with Action and collects the Reward. The Enviornment transitions from an older state to a new state in reponse to the action by the Agent. . State Value and Action Value . Policy defines the learning agents way of behaving at a given time.A policy is a Mapping from States to probability of selecting different actions.Its denoted by π piπ. . If the agent is following policy π piπ at time t , then π(a∣s) pi(a mid s)π(a∣s) is the probability that action AtA_{t}At​ = a if state,StS_{t}St​ =s. . Reward is the goal of the RL Problem. The objective of the agent is to maximize the total rewards over time. . The State Value for a state sss under policy π piπ denoted as vπ(s)v_{ pi}(s)vπ​(s) ,is defined as the Expected return when starting in sss and following policy π piπ thereafter. . vπ(s)=Eπ[Gt∣St=s]=Eπ[∑k=0∞γkRt+k+1∣St=s]v_{ pi}(s) = mathbb{E}_{ pi} left[ G_{t} | S_{t}=s right] = mathbb{E}_{ pi} left[ sum_{k=0 }^{ infty} gamma^{k}R_{t+k+1}|S_{t}=s right]vπ​(s)=Eπ​[Gt​∣St​=s]=Eπ​[k=0∑∞​γkRt+k+1​∣St​=s] . for all s ∈ in∈ S. . The value of the state is the total amount of reward which the agent can accumulate over time. Each state s has a value v(s) . . State Value . State1 | v(s1)v(s1)v(s1) | . State2 | v(s2)v(s2)v(s2) | . State3 | v(s3)v(s3)v(s3) | . State4 | v(s4)v(s4)v(s4) | . Action value denoted as Q(s,a)Q(s,a)Q(s,a) is the expected return starting from state s, taking an action a and then following policy π piπ. . qπ(s,a)=Eπ[Gt∣St=s,At=a]=Eπ[∑k=0∞γkRt+k+1∣St=s,At=a]q_{ pi}(s,a) = mathbb{E}_{ pi} left[ G_{t} | S_{t}=s ,A_{t}=a right] = mathbb{E}_{ pi} left[ sum_{k=0 }^{ infty} gamma^{k}R_{t+k+1}|S_{t}=s ,A_{t}=a right]qπ​(s,a)=Eπ​[Gt​∣St​=s,At​=a]=Eπ​[k=0∑∞​γkRt+k+1​∣St​=s,At​=a] . We have qvalues for each state for all the possible actions . . Lets say , we have four states and each state has three possibe actions ,then Q values can be shown in the tabular form . State/Actions Action1 Action2 Action3 . State1 | Q(s1,a1)Q(s1,a1)Q(s1,a1) | Q(s1,a2)Q(s1,a2)Q(s1,a2) | Q(s1,a3)Q(s1,a3)Q(s1,a3) | . State2 | Q(s2,a1)Q(s2,a1)Q(s2,a1) | Q(s2,a2)Q(s2,a2)Q(s2,a2) | Q(s2,a3)Q(s2,a3)Q(s2,a3) | . State3 | Q(s3,a1)Q(s3,a1)Q(s3,a1) | Q(s3,a2)Q(s3,a2)Q(s3,a2) | Q(s3,a3)Q(s3,a3)Q(s3,a3) | . State4 | Q(s4,a1)Q(s4,a1)Q(s4,a1) | Q(s4,a2)Q(s4,a2)Q(s4,a2) | Q(s4,a3)Q(s4,a3)Q(s4,a3) | . As shown in the table above There is a action value QQQ for all the actions in a state unlike the Value function VVV that has a value just for each state.When the model is not available, the Q values helps to choose the next best action from that state. . Model Based Vs Model Free . Model Free means that we do not learn the model of the enviornment. We do not learn complete map of the domain. We do not learn the Probability Transition from one state to another state. P(S′∣S,a)P(S^{&amp;#x27;}| S,a)P(S′∣S,a). . Although we need a model , we need the model to generate only sample transitions, not the complete probability distributions of all possible transitions. . At the end of the learning in model free methods, we won’t have transition probabilities of the enviornment neither we know the rewards we will get without taking that action. However a policy alone is learnt based on the model(eg.Policy Gradient , Q learning) . On Policy Vs Off Policy Methods . On-Policy methods attempt to improve the policy that is used to make decisions. | Off-Policy methods evaluate or improve a policy different from the one that is used to make decisions. | . In On-Policy methods - the Policy that is being learnt and the policy that is used to explore ,move to next state are the same. . In off policy methods, two different policies are used. . Target policy - The policy that is being learned about - This will be the optimal policy. | Behaviour policy - The policy that is used to generate behaviour - This will be more exploratory to explore more actions. | Q learning is an off policy TD control as the policy we use to estimate qvalues and the policy that is used to take actions are different. . Temporal difference Learning . TD stands for Temporal difference Learning. . TD is a hybrid of both the Montecarlo and Dynamic programming. . Like MonteCarlo, its Model free.(ie) TD methods can learn directly from raw experience without a model of the enviornments dynamics. | Like Dynamic Programming , they bootstrap , meaning they update estimates based on other estimate and doesn’t wait for the END of the episode to update. | TD uses sample updates like Montecarlo | . It involves . From a state ,look ahead of the sampled sucessor state, | Compute a Backed up value, using the value of sucessor state and the reward collected along the way , | Update the value of the state accordingly | . The simplest TD method makes the update: . V(St)←V(St)+α[Rt+1+γV(St+1)−V(St)]V({S_t}) leftarrow V({S_t}) + alpha left[{R_{t+1}} + gamma V({S_{t+1}}) - V({S_t}) right]V(St​)←V(St​)+α[Rt+1​+γV(St+1​)−V(St​)] . Here V(St)V({S_t})V(St​) is the estimated value and V(St+1)V({S_{t+1}})V(St+1​) is the successor state , Rt+1{R_{t+1}}Rt+1​ is the reward collected and the computed backedup value is Rt+1+γV(St+1){R_{t+1}} + gamma V({S_{t+1}})Rt+1​+γV(St+1​) . The Error between the estimated value of StS_{t}St​ and the better estimate (Rt+1+γV(St+1)−V(St))({R_{t+1}} + gamma V({S_{t+1}}) - V({S_t}))(Rt+1​+γV(St+1​)−V(St​)) is called TD error δ deltaδ. . δt=Rt+1+γV(St+1)−V(St) delta_{t} = {R_{t+1}} + gamma V({S_{t+1}}) - V({S_t})δt​=Rt+1​+γV(St+1​)−V(St​) . More Genericially , TD Update can be written as . NewEstimate←OldEstimate+stepsize[Target−OldEstimate]NewEstimate leftarrow OldEstimate + stepsize left[ Target - OldEstimate right]NewEstimate←OldEstimate+stepsize[Target−OldEstimate] . When the model is not available we will be using Action values. (because to calculate state value, we need probability transions) . The TD(0) update for action values is : . Q(St,At)←Q(St,At)+α[Rt+1+γQ(St+1,At+1)−Q(St,At)]Q({S_t},{A_t}) leftarrow Q({S_t},{A_t}) + alpha left[{R_{t+1}} + gamma Q({S_{t+1}},{A_{t+1}}) - Q({S_t},{A_t}) right]Q(St​,At​)←Q(St​,At​)+α[Rt+1​+γQ(St+1​,At+1​)−Q(St​,At​)] . Q-Learning: . Q learning is an off policy TD Control. . The equation for Q learning is . Q(St,At)←Q(St,At)+α[Rt+1+γmaxaQ(St+1,a)−Q(St,At)]Q({S_t},{A_t}) leftarrow Q({S_t},{A_t}) + alpha left[{R_{t+1}} + gamma underset{a}{max} Q({S_{t+1}},a) - Q({S_t},{A_t}) right]Q(St​,At​)←Q(St​,At​)+α[Rt+1​+γamax​Q(St+1​,a)−Q(St​,At​)] . In Q learning ,we force the target policy Q(S,A)Q(S,A)Q(S,A) to move towards the optimal q∗q^{*}q∗ by acting greedily (maxaQ(St+1,a) underset{a}{max} Q({S_{t+1}},a)amax​Q(St+1​,a)) in that state . We dont follow that greedy action .The update is made assuming we follow the greedy behaviour.-Similiar to asking the question.. What is the estimate of Q(St,At)Q({S_t},{A_t})Q(St​,At​) if we take a greedy action at this state St+1{S_{t+1}}St+1​.We collect rewards Rt+1R_{t+1}Rt+1​ for the initial action At{A_{t}}At​ and compute the backedup action values for Q(St+1,a)Q({S_{t+1}},a)Q(St+1​,a), and use it to estimate the values at Q(St)Q({S_t})Q(St​) . Since both the policies are different , Q learning is Off policy . The Q learning Algorithm . . It should be clear from above that , for each estimate of Q at state StS_{t}St​ , Q learning uses the maximum action value of the the state St+1S_{t+1}St+1​ (Highlighted in blue).However, the next action is always derived by a different policy such as ϵ epsilonϵ-greedy (Highlighted in Green) . Q Learning Vs SARSA: . SARSA . SARSA is defined as on-policy TD Control to find optimal policy. It stands for the transition given by State,Action,Reward,(next)State,(next)Action - SARSA . The sarsa update equation is given as . Q(St,At)←Q(St,At)+α[Rt+1+γQ(St+1,At+1)−Q(St,At)]Q({S_t},{A_t}) leftarrow Q({S_t},{A_t}) + alpha left[{R_{t+1}} + gamma Q({S_{t+1}},{A_{t+1}}) - Q({S_t},{A_t}) right]Q(St​,At​)←Q(St​,At​)+α[Rt+1​+γQ(St+1​,At+1​)−Q(St​,At​)] . We use the backed up action values at next state Q(St+1,At+1)Q({S_{t+1}},{A_{t+1}})Q(St+1​,At+1​) to re-estimate the values at state Q(St,At)Q({S_t},{A_t})Q(St​,At​) . In SARSA the updates are made assuming we follow the actions defined by the policy - There is no seperate greedy policy for the updates. as in Q learning. The policy used to make the update and the one used to pick next action is the same.Thus SARSA is online. . From the state St{S_t}St​,the agent chooses At{A_t}At​ , collects a reward Rt+1{R_{t+1}}Rt+1​ , and goes to the next state St+1{S_{t+1}}St+1​. The agent chooses the next action At+1{A_{t+1}}At+1​ based on its policy .We collect the action values at Q(St+1,At+1)Q({S_{t+1}},{A_{t+1}})Q(St+1​,At+1​) and use it to estimate action values of the former state action Q(St,At)Q({S_t},{A_t})Q(St​,At​) . . The sarsa Algorithm . . In the line highlighted with Blue, the Agent chooses an action A’ from next state state S’ state using the greedy policy to update the current state S. In the final line A←A′A leftarrow A^{&amp;#x27;}A←A′ , the same action A’ selected previously (by ϵ epsilonϵ-greedy policy) for the update is used by the Agent to transition to the next state . . Case Study : Cliff Walker Enviornment . Here we shall find a case study with Cliff Walker Enviornment . Here is the jupyter notebook comparision of Q learning and SARSA for the cliff walker enviornment. . ` This is a simple implementation of the Gridworld Cliff reinforcement learning task. . Adapted from Example 6.6 (page 132) from Reinforcement Learning: An Introduction by Sutton and Barto: http://incompleteideas.net/book/RLbook2020.pdf With inspiration from: https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/cliff_walking.py The board is a 4x12 matrix, with (using Numpy matrix indexing): [3, 0] as the start at bottom-left [3, 11] as the goal at bottom-right [3, 1..10] as the cliff at bottom-center Each time step incurs -1 reward, and stepping into the cliff incurs -100 reward and a reset to the start. An episode terminates when the agent reaches the goal.` . The enviornment is shown below . o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o S C C C C C C C C C C G . Here, . S is the only starting point. | G is the Target Goal. | C is Cliff . | Stepping into C causes Reward to be -100. | Stepping into C causes agent to restart from S | Reward is -1 on on all other transitions. | . We will run both the Q learning and Sarsa on this enviornment to compare their performances. . We could see that Q learning learns the best policy after an inital transient. . . Policy Comparision . The policy learnt by both the Q learning and SARSA can be compared. . Q-Learning Actions . &gt; &gt; &gt; &gt; &gt; &gt; &gt; v v &gt; v v &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; v &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; v S C C C C C C C C C C G . Here we can see from the starting point S, the policy goes to the immediate right and reaches the Goal. . SARSA Actions . &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; v ^ ^ ^ &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; v ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ &gt; v S C C C C C C C C C C G . Here we can see , the agent goes to the top of the grid , takes a right and then reaches the Goal . . SARSA takes the longer, safer path going to the top and then moving towards the Goal. However, Q learning takes the best action and takes the path near to the cliff, the optimal path (shortest) . sarsa is an onpolicy algorithm and it cannot afford to fall the cliff every time. So it takes a much safer path. . Summary . First we saw the RL setup , some basic blocks in RL and then finally we compared Q learning with Sarsa. I just scratched the surface here.There is so much to explore in RL. I hope this will give a small boost for the beginner to dive deep to Reinforcement Learning . . Keep Learning ! Start DOING !! . Useful resources . Book:Reinforcement Learning An introduction by Sutton &amp; Barto | OnPolicy Vs Off Policy | Modelbased Vs ModelFree- from Reddit | Modelbased Vs ModelFree- from Quora | RL Intro- KD Nuggets | Q-learning Vs SARSA | Distill - Understanding TD | RL Intro -lilianweng.github.io | Good RL Series by Massimiliano |",
            "url": "https://mniju.github.io/Blog/reinforcement%20learning/2020/05/01/Qlearning-SARSA.html",
            "relUrl": "/reinforcement%20learning/2020/05/01/Qlearning-SARSA.html",
            "date": " • May 1, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://mniju.github.io/Blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://mniju.github.io/Blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Niju Michael Nicholas.I am an Electrical Engineer by graduation , by profession an Instrumentation and Test Automation Engineer . An avid AI enthusiasist and more specifically into RL where Robotics meets Artificial Intelligence. I occassionaly blog here when i organise some notes when stumble on new ideas/subjects. You can reach me at : niju.nicholas@gmail.com .",
          "url": "https://mniju.github.io/Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mniju.github.io/Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}